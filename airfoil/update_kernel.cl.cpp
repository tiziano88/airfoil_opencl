//
// auto-generated by op2.m on 30-May-2011 22:03:11
//

// user function

//#include "update.h"

// host stub function

void op_par_loop_update(char const *name, op_set set,
  op_arg arg0,
  op_arg arg1,
  op_arg arg2,
  op_arg arg3,
  op_arg arg4 ){

  float *arg4h = (float *)arg4.data;

  cl_int ciErrNum;

  if (OP_diags>2) {
    printf(" kernel routine w/o indirection:  update \n");
  }

  // initialise timers

  double cpu_t1, cpu_t2, wall_t1, wall_t2;
  op_timers(&cpu_t1, &wall_t1);

  // set CUDA execution parameters

  #ifdef OP_BLOCK_SIZE_4
    const size_t nthread = OP_BLOCK_SIZE_4;
  #else
    // int nthread = OP_block_size;
    const size_t nthread = 128;
  #endif

  const size_t nblocks = 200;
  const size_t n_tot_thread = nblocks * nthread;

  // transfer global reduction data to GPU

  int maxblocks = nblocks;

  int reduct_bytes = 0;
  int reduct_size  = 0;
  reduct_bytes += ROUND_UP(maxblocks*1*sizeof(float));
  reduct_size   = MAX(reduct_size,sizeof(float));

  reallocReductArrays(reduct_bytes);

  reduct_bytes = 0;
  arg4.data   = OP_reduct_h + reduct_bytes;
  arg4.data_d = (char *) OP_reduct_d; //+ reduct_bytes; //instead pass offset to kernel
  int arg4_offset = reduct_bytes;

  for (int b=0; b<maxblocks; b++)
    for (int d=0; d<1; d++)
      ((float *)arg4.data)[d+b*1] = ZERO_float;
  reduct_bytes += ROUND_UP(maxblocks*1*sizeof(float));

  mvReductArraysToDevice(reduct_bytes);

  // work out shared memory requirements per element

  int nshared = 0;
  nshared = MAX(nshared,sizeof(float)*4);
  nshared = MAX(nshared,sizeof(float)*4);
  nshared = MAX(nshared,sizeof(float)*4);

  // execute plan

  int offset_s = nshared*OP_WARPSIZE;

  nshared = MAX(nshared*nthread,reduct_size*nthread);

  cl_kernel hKernel = getKernel( "op_cuda_update" );

  //offset_s *= 4;
  //nshared *= 4;


  int i = 0;
  ciErrNum = clSetKernelArg( hKernel, i++, sizeof(cl_mem), &(arg0.data_d) );
  ciErrNum |= clSetKernelArg( hKernel, i++, sizeof(cl_mem), &(arg1.data_d) );
  ciErrNum |= clSetKernelArg( hKernel, i++, sizeof(cl_mem), &(arg2.data_d) );
  ciErrNum |= clSetKernelArg( hKernel, i++, sizeof(cl_mem), &(arg3.data_d) );
  ciErrNum |= clSetKernelArg( hKernel, i++, sizeof(cl_mem), &(arg4.data_d) );
  ciErrNum |= clSetKernelArg( hKernel, i++, sizeof(int), &arg4_offset );
  ciErrNum |= clSetKernelArg( hKernel, i++, sizeof(int), &offset_s );
  ciErrNum |= clSetKernelArg( hKernel, i++, sizeof(int), &set->size );
  ciErrNum |= clSetKernelArg( hKernel, i++, nshared, NULL );
  assert_m( ciErrNum == CL_SUCCESS, "error setting kernel arguments" );

  ciErrNum = clEnqueueNDRangeKernel( cqCommandQueue, hKernel, 1, NULL, &n_tot_thread, &nthread, 0, NULL, NULL );
  assert_m( ciErrNum == CL_SUCCESS, "error executing kernel" );

  ciErrNum = clFinish( cqCommandQueue );
  assert_m( ciErrNum == CL_SUCCESS, "error completing device commands" );
  
  // transfer global reduction data back to CPU

  mvReductArraysToHost(reduct_bytes);

  for (int b=0; b<maxblocks; b++)
    for (int d=0; d<1; d++)
      arg4h[d] = arg4h[d] + ((float *)arg4.data)[d+b*1];

  // update kernel record

  op_timers(&cpu_t2, &wall_t2);
  op_timing_realloc(4);
  OP_kernels[4].name      = name;
  OP_kernels[4].count    += 1;
  OP_kernels[4].time     += wall_t2 - wall_t1;
  OP_kernels[4].transfer += (float)set->size * arg0.size;
  OP_kernels[4].transfer += (float)set->size * arg1.size;
  OP_kernels[4].transfer += (float)set->size * arg2.size * 2.0f;
  OP_kernels[4].transfer += (float)set->size * arg3.size;
}


